# Specify server locations in a SOLR_LOCATOR variable;
# used later in variable substitutions
# Change the zkHost to point to your own Zookeeper quorum
SOLR_LOCATOR : {
    # Name of solr collection
    collection : email_collection
    # ZooKeeper ensemble
    zkHost : "clust2:2181/solr"
}

# Specify an array of one or more morphlines, each of which defines an ETL
# transformation chain. A morphline consists of one or more (potentially
# nested) commands. A morphline is a way to consume records (e.g. Flume events,
# HDFS files or blocks), turn them into a stream of records, and pipe the stream
# of records through a set of easily configurable transformations on it's way to
# Solr (or a MapReduceIndexerTool RecordWriter that feeds via a Reducer into Solr).
morphlines : [
{
    # Name used to identify a morphline. E.g. used if there are multiple morphlines in a
    # morphline config file
    id : morphline1
    # Import all morphline commands in these java packages and their subpackages.
    # Other commands that may be present on the classpath are not visible to this morphline.
    importCommands : ["com.cloudera.**", "org.apache.solr.**"]
    commands : [
    {
        ## Read the email stream and break it up into individual messages.
        ## The beginning of a message is marked by regex clause below
        ## The reason we use this command is that one event can have multiple
        ## messages
        readMultiLine {
            regex : "Message-ID:.*"
            what : next
            charset : UTF-8
        }
    }
    ## Break up the email text into SOLR fields
    {
        if {
            conditions: [
            {
                not{
                    grok {
                        expressions : {
                            message: """(?s)(.*?)(Message-ID: &lt;)(?&lt;message_id&gt;(.+?))(&gt;.*?)(Date: )(?&lt;date&gt;(.+?))( \(.+?)(From: )(?&lt;from&gt;(.*?))(To: )(?&lt;to&gt;(.+?))(Subject: )(?&lt;subject&gt;(.*?))((Cc: )(?&lt;cc&gt;(.*)))?(Mime.+?)((Bcc: )(?&lt;bcc&gt;(.*)))?(X-From: )(?&lt;from_names&gt;(.*?))(X-To: )(?&lt;to_names&gt;(.*?))(X-cc: )(?&lt;cc_names&gt;(.*?))(X-bcc: )(?&lt;bcc_names&gt;(.*?))(X-Folder: )(?&lt;x_folder&gt;(.*?))(X-Origin: )(?&lt;x_origin&gt;(.*?))(X-FileName: )(?&lt;x_filename&gt;(.*?))(\n)(?&lt;body&gt;(.*))"""
                        }
                        extract: inplace
                        findSubstrings: false
                        addEmptyStrings: false
                        numRequiredMatches: all
                    }
                }
            }
            ]
            then:[
            { logInfo { format : "found no grok match: {}", args : ["@{}"] } }
            { dropRecord {} }
            ]
        }
    }

    # add Unique ID, in case our message_id field from above is not present
    {
        generateUUID {
            field:message_id
        }
    }

    # convert the timestamp field to "yyyy-MM-dd'T'HH:mm:ss.SSSZ" format
    {
        convertTimestamp {
            field : date
            inputFormats : ["EEE, d MMM yyyy HH:mm:ss Z", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", "yyyy-MM-dd'T'HH:mm:ss", "yyyy-MM-dd"]
            inputTimezone : America/Los_Angeles
            outputFormat : "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
            outputTimezone : UTC
        }
    }

    # Consume the output record of the previous command and pipe another
    # record downstream.
    #
    # This command sanitizes record fields that are unknown to Solr schema.xml
    # by deleting them. Recall that Solr throws an exception on any attempt to
    # load a document that contains a field that isn't specified in schema.xml
    {
        sanitizeUnknownSolrFields {
            # Location from which to fetch Solr schema
            solrLocator : ${SOLR_LOCATOR}
        }
    }

    # load the record into a SolrServer or MapReduce SolrOutputFormat.
    {
        loadSolr {
            solrLocator : ${SOLR_LOCATOR}
        }
    }
    ]
}
]
